{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18520440_VoQuocAn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anvq38/CS114.K21.KHTN/blob/master/18520440_VoQuocAn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx5BaVD40IFj",
        "colab_type": "code",
        "outputId": "d70515d6-756e-4dd1-adb6-c23df946d80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# FrozenLake-v0\n",
        "import numpy as np\n",
        "import gym\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "env.reset()\n",
        "\n",
        "def value_iteration(env, max_iters, gamma=0.9):\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Compute value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # Compute q-value for each action\n",
        "            for action in range(env.action_space.n):                \n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # Select the best action\n",
        "            best_action = np.argmax(np.asarray(q_values))\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print('Converged at {}-th iteration.'.format(i))\n",
        "            break\n",
        "    \n",
        "    return v_values\n",
        "\n",
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "    \n",
        "    # Compute the best action for each state in the game\n",
        "    # Compute q-values for each (state-action) pair in the game\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "\n",
        "        # Compute q-values for each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # Select the best action\n",
        "        best_action = np.argmax(np.asarray(q_values))\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy\n",
        "\n",
        "v_values = value_iteration(env, max_iters=1000, gamma=0.9)\n",
        "\n",
        "policy = policy_extraction(env, v_values, gamma=0.9)\n",
        "\n",
        "print(policy)\n",
        "\n",
        "env.render()\n",
        "\n",
        "def play(env, policy):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        steps += 1\n",
        "        #env.render()\n",
        "        state = next_state\n",
        "    \n",
        "    #print('# steps: {}'.format(steps))\n",
        "    return (reward, steps)\n",
        "\n",
        "def play_multiple_times(env, policy):\n",
        "    num_episodes = 1000\n",
        "    list_of_steps = []\n",
        "    num_failures = 0\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        reward, steps = play(env, policy)\n",
        "        if reward == 1:\n",
        "            list_of_steps.append(steps)\n",
        "        else:\n",
        "            num_failures += 1\n",
        "\n",
        "    print('# failures: {}/{}'.format(num_failures, num_episodes))\n",
        "    print('avg. # steps: {}'.format(np.mean(list_of_steps)))\n",
        "\n",
        "play_multiple_times(env, policy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converged at 79-th iteration.\n",
            "[0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "# failures: 263/1000\n",
            "avg. # steps: 36.86024423337856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTxbIPzx0Flw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUlIK0Se0kOp",
        "colab_type": "code",
        "outputId": "0627a837-5c27-4576-a643-5ab0e532f0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# FrozenLake8x8-v0\n",
        "import numpy as np\n",
        "import gym\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "\n",
        "env.reset()\n",
        "\n",
        "def value_iteration(env, max_iters, gamma=0.9):\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Compute value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # Compute q-value for each action\n",
        "            for action in range(env.action_space.n):                \n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # Select the best action\n",
        "            best_action = np.argmax(np.asarray(q_values))\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print('Converged at {}-th iteration.'.format(i))\n",
        "            break\n",
        "    \n",
        "    return v_values\n",
        "\n",
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "    \n",
        "    # Compute the best action for each state in the game\n",
        "    # Compute q-values for each (state-action) pair in the game\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "\n",
        "        # Compute q-values for each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # Select the best action\n",
        "        best_action = np.argmax(np.asarray(q_values))\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy\n",
        "\n",
        "v_values = value_iteration(env, max_iters=1000, gamma=0.9)\n",
        "\n",
        "policy = policy_extraction(env, v_values, gamma=0.9)\n",
        "\n",
        "print(policy)\n",
        "\n",
        "env.render()\n",
        "\n",
        "def play(env, policy):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        steps += 1\n",
        "        #env.render()\n",
        "        state = next_state\n",
        "    \n",
        "    #print('# steps: {}'.format(steps))\n",
        "    return (reward, steps)\n",
        "\n",
        "def play_multiple_times(env, policy):\n",
        "    num_episodes = 1000\n",
        "    list_of_steps = []\n",
        "    num_failures = 0\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        reward, steps = play(env, policy)\n",
        "        if reward == 1:\n",
        "            list_of_steps.append(steps)\n",
        "        else:\n",
        "            num_failures += 1\n",
        "\n",
        "    print('# failures: {}/{}'.format(num_failures, num_episodes))\n",
        "    print('avg. # steps: {}'.format(np.mean(list_of_steps)))\n",
        "\n",
        "play_multiple_times(env, policy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converged at 117-th iteration.\n",
            "[3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0]\n",
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "# failures: 254/1000\n",
            "avg. # steps: 73.1970509383378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFtyirawB66o",
        "colab_type": "text"
      },
      "source": [
        "Ưu điểm của thuật toán Value Iteration\n",
        "+ Có thể tìm được node goal\n",
        "+ Đưa ra được hành động tối ưu ở mỗi trạng thái\n",
        "+ Dễ thực hiện\n",
        "\n",
        "Nhược điểm của thuật toán Value Iteration\n",
        "+ Có thể không tới được node goal\n",
        "+ Độ phức tạp của thuật toán lớn\n",
        "+ Cần một số lần lặp nhất định để đạt trạng thái hội tụ\n",
        "\n",
        "Những trường hợp thuật toán Value Iteration không hoạt động hiệu quả\n",
        "+ Độ hữu dụng của các trạng thái bằng nhau\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1VJYerS0zT-",
        "colab_type": "code",
        "outputId": "4d895e79-ca37-4e8e-b9db-b79da6d4e5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "# Taxi-v3\n",
        "import numpy as np\n",
        "import gym\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "env.reset()\n",
        "\n",
        "def value_iteration(env, max_iters, gamma=0.9):\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Compute value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # Compute q-value for each action\n",
        "            for action in range(env.action_space.n):                \n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # Select the best action\n",
        "            best_action = np.argmax(np.asarray(q_values))\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print('Converged at {}-th iteration.'.format(i))\n",
        "            break\n",
        "    \n",
        "    return v_values\n",
        "\n",
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "    \n",
        "    # Compute the best action for each state in the game\n",
        "    # Compute q-values for each (state-action) pair in the game\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "\n",
        "        # Compute q-values for each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # Select the best action\n",
        "        best_action = np.argmax(np.asarray(q_values))\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy\n",
        "\n",
        "v_values = value_iteration(env, max_iters=1000, gamma=0.9)\n",
        "\n",
        "policy = policy_extraction(env, v_values, gamma=0.9)\n",
        "\n",
        "print(policy)\n",
        "\n",
        "env.render()\n",
        "\n",
        "def play(env, policy):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        steps += 1\n",
        "        #env.render()\n",
        "        state = next_state\n",
        "    \n",
        "    #print('# steps: {}'.format(steps))\n",
        "    return (reward, steps)\n",
        "\n",
        "def play_multiple_times(env, policy):\n",
        "    num_episodes = 1000\n",
        "    list_of_steps = []\n",
        "    num_failures = 0\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        reward, steps = play(env, policy)\n",
        "        if reward > 0:\n",
        "            list_of_steps.append(steps)\n",
        "        else:\n",
        "            num_failures += 1\n",
        "\n",
        "    print('# failures: {}/{}'.format(num_failures, num_episodes))\n",
        "    print('avg. # steps: {}'.format(np.mean(list_of_steps)))\n",
        "\n",
        "play_multiple_times(env, policy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converged at 116-th iteration.\n",
            "[4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3]\n",
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "# failures: 0/1000\n",
            "avg. # steps: 13.146\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}