{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18520440_VoQuocAn_PolicyIteration.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3/GWwAhWqUN0RrELCMEnC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anvq38/CS114.K21.KHTN/blob/master/18520440_VoQuocAn_PolicyIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLc-SSWgmvwM",
        "colab_type": "code",
        "outputId": "a710de64-a5c7-4834-99ad-b5ebab6c0af9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "def policy_evaluation(policy, gamma=0.9):\n",
        "    \n",
        "    # initialize value table with zeros\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        # store prev_v_values \n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # for each state, compute the value according to the policy\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob* (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value\n",
        "            \n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            break\n",
        "            \n",
        "    return v_values\n",
        "\n",
        "def policy_extraction(v_values, gamma = 0.9):\n",
        " \n",
        "    # Initialize the policy with zeros\n",
        "    policy = np.zeros(env.observation_space.n) \n",
        "    \n",
        "    for state in range(env.observation_space.n):\n",
        "        \n",
        "        # initialize the q_values for a state\n",
        "        q_values = np.zeros(env.action_space.n)\n",
        "        \n",
        "        # compute q_value for all ations in the state\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob* (reward + gamma * v_values[next_state])\n",
        "            q_values[action] = q_value\n",
        "        # Select the best action\n",
        "        policy[state] = np.argmax(q_values)\n",
        "    \n",
        "    return policy\n",
        "\n",
        "def policy_iteration(env,max_iters,gamma = 0.9):\n",
        "    \n",
        "    # Initialize policy with zeros\n",
        "    #old_policy = np.zeros(env.observation_space.n)\n",
        "       \n",
        "    old_policy = np.random.choice(env.env.nA, size=(env.observation_space.n))\n",
        "    \n",
        "    for i in range(max_iters):\n",
        "        \n",
        "        # compute new_value from policy_evaluation function\n",
        "        new_value = policy_evaluation(old_policy, gamma)\n",
        "        \n",
        "        # Extract new policy from policy_extraction function\n",
        "        new_policy = policy_extraction(new_value, gamma)\n",
        "\n",
        "        if (np.all(old_policy == new_policy)):\n",
        "            print('Coverged at {}-th iteration.'.format(i+1))\n",
        "            break\n",
        "        old_policy = new_policy\n",
        "        \n",
        "    return new_policy\n",
        "\n",
        "policy = policy_iteration(env,max_iters=1000,gamma=0.9)\n",
        "print (policy)\n",
        "\n",
        "def play(env, policy):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        steps += 1\n",
        "        state = next_state\n",
        "    \n",
        "    return (reward, steps)\n",
        "\n",
        "def play_multiple_times(env, policy):\n",
        "    num_episodes = 1000\n",
        "    list_of_steps = []\n",
        "    num_failures = 0\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        reward, steps = play(env, policy)\n",
        "        if reward == 1:\n",
        "            list_of_steps.append(steps)\n",
        "        else:\n",
        "            num_failures += 1\n",
        "\n",
        "    print('# failures: {}/{}'.format(num_failures, num_episodes))\n",
        "    print('avg. # steps: {}'.format(np.mean(list_of_steps)))\n",
        "\n",
        "play_multiple_times(env, policy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Coverged at 5-th iteration.\n",
            "[0. 3. 0. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n",
            "# failures: 261/1000\n",
            "avg. # steps: 36.56833558863329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuVa4OMRm4yP",
        "colab_type": "code",
        "outputId": "f020ecf4-ade0-45e5-a6f7-945923f54b21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "def policy_evaluation(policy, gamma=0.9):\n",
        "    \n",
        "    # initialize value table with zeros\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        # store prev_v_values \n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # for each state, compute the value according to the policy\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob* (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value\n",
        "            \n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            break\n",
        "            \n",
        "    return v_values\n",
        "\n",
        "def policy_extraction(v_values, gamma = 0.9):\n",
        " \n",
        "    # Initialize the policy with zeros\n",
        "    policy = np.zeros(env.observation_space.n) \n",
        "    \n",
        "    for state in range(env.observation_space.n):\n",
        "        \n",
        "        # initialize the q_values for a state\n",
        "        q_values = np.zeros(env.action_space.n)\n",
        "        \n",
        "        # compute q_value for all ations in the state\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob* (reward + gamma * v_values[next_state])\n",
        "            q_values[action] = q_value\n",
        "        # Select the best action\n",
        "        policy[state] = np.argmax(q_values)\n",
        "    \n",
        "    return policy\n",
        "\n",
        "def policy_iteration(env,max_iters,gamma = 0.9):\n",
        "    \n",
        "    # Initialize policy with zeros\n",
        "    #old_policy = np.zeros(env.observation_space.n)\n",
        "       \n",
        "    old_policy = np.random.choice(env.env.nA, size=(env.observation_space.n))\n",
        "    \n",
        "    for i in range(max_iters):\n",
        "        \n",
        "        # compute new_value from policy_evaluation function\n",
        "        new_value = policy_evaluation(old_policy, gamma)\n",
        "        \n",
        "        # Extract new policy from policy_extraction function\n",
        "        new_policy = policy_extraction(new_value, gamma)\n",
        "\n",
        "        if (np.all(old_policy == new_policy)):\n",
        "            print('Coverged at {}-th iteration.'.format(i+1))\n",
        "            break\n",
        "        old_policy = new_policy\n",
        "        \n",
        "    return new_policy\n",
        "\n",
        "policy = policy_iteration(env,max_iters=1000,gamma=0.9)\n",
        "print (policy)\n",
        "\n",
        "def play(env, policy):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        steps += 1\n",
        "        state = next_state\n",
        "    \n",
        "    return (reward, steps)\n",
        "\n",
        "def play_multiple_times(env, policy):\n",
        "    num_episodes = 1000\n",
        "    list_of_steps = []\n",
        "    num_failures = 0\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        reward, steps = play(env, policy)\n",
        "        if reward == 1:\n",
        "            list_of_steps.append(steps)\n",
        "        else:\n",
        "            num_failures += 1\n",
        "\n",
        "    print('# failures: {}/{}'.format(num_failures, num_episodes))\n",
        "    print('avg. # steps: {}'.format(np.mean(list_of_steps)))\n",
        "\n",
        "play_multiple_times(env, policy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Coverged at 4-th iteration.\n",
            "[3. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 2. 2. 2. 1. 3. 3. 0. 0. 2. 3. 2. 1.\n",
            " 3. 3. 3. 1. 0. 0. 2. 1. 3. 3. 0. 0. 2. 1. 3. 2. 0. 0. 0. 1. 3. 0. 0. 2.\n",
            " 0. 0. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 1. 1. 1. 0.]\n",
            "# failures: 283/1000\n",
            "avg. # steps: 72.50627615062761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeV4Rov2nFQ5",
        "colab_type": "code",
        "outputId": "f98194cd-a215-4352-b4e7-d195e992b6a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "def policy_evaluation(policy, gamma=0.9):\n",
        "    \n",
        "    # initialize value table with zeros\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        # store prev_v_values \n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # for each state, compute the value according to the policy\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob* (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value\n",
        "            \n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            break\n",
        "            \n",
        "    return v_values\n",
        "\n",
        "def policy_extraction(v_values, gamma = 0.9):\n",
        " \n",
        "    # Initialize the policy with zeros\n",
        "    policy = np.zeros(env.observation_space.n) \n",
        "    \n",
        "    for state in range(env.observation_space.n):\n",
        "        \n",
        "        # initialize the q_values for a state\n",
        "        q_values = np.zeros(env.action_space.n)\n",
        "        \n",
        "        # compute q_value for all ations in the state\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob* (reward + gamma * v_values[next_state])\n",
        "            q_values[action] = q_value\n",
        "        # Select the best action\n",
        "        policy[state] = np.argmax(q_values)\n",
        "    \n",
        "    return policy\n",
        "\n",
        "def policy_iteration(env,max_iters,gamma = 0.9):\n",
        "    \n",
        "    # Initialize policy with zeros\n",
        "    #old_policy = np.zeros(env.observation_space.n)\n",
        "       \n",
        "    old_policy = np.random.choice(env.env.nA, size=(env.observation_space.n))\n",
        "    \n",
        "    for i in range(max_iters):\n",
        "        \n",
        "        # compute new_value from policy_evaluation function\n",
        "        new_value = policy_evaluation(old_policy, gamma)\n",
        "        \n",
        "        # Extract new policy from policy_extraction function\n",
        "        new_policy = policy_extraction(new_value, gamma)\n",
        "\n",
        "        if (np.all(old_policy == new_policy)):\n",
        "            print('Coverged at {}-th iteration.'.format(i+1))\n",
        "            break\n",
        "        old_policy = new_policy\n",
        "        \n",
        "    return new_policy\n",
        "\n",
        "policy = policy_iteration(env,max_iters=1000,gamma=0.9)\n",
        "print (policy)\n",
        "\n",
        "def play(env, policy):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        steps += 1\n",
        "        state = next_state\n",
        "    \n",
        "    return (reward, steps)\n",
        "\n",
        "def play_multiple_times(env, policy):\n",
        "    num_episodes = 1000\n",
        "    list_of_steps = []\n",
        "    num_failures = 0\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        reward, steps = play(env, policy)\n",
        "        if reward > 0:\n",
        "            list_of_steps.append(steps)\n",
        "        else:\n",
        "            num_failures += 1\n",
        "\n",
        "    print('# failures: {}/{}'.format(num_failures, num_episodes))\n",
        "    print('avg. # steps: {}'.format(np.mean(list_of_steps)))\n",
        "\n",
        "play_multiple_times(env, policy)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Coverged at 18-th iteration.\n",
            "[4. 4. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 3. 3. 3. 3.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 4. 4. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 5. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 2. 2.\n",
            " 1. 2. 0. 2. 1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3. 2. 2. 2. 2. 1. 2. 3. 2.\n",
            " 3. 3. 3. 3. 1. 1. 1. 1. 3. 3. 3. 3. 2. 2. 2. 2. 3. 1. 3. 2. 3. 3. 3. 3.\n",
            " 1. 1. 1. 1. 3. 3. 3. 3. 0. 0. 0. 0. 3. 1. 3. 0. 3. 3. 3. 3. 1. 1. 1. 1.\n",
            " 3. 3. 3. 3. 0. 0. 0. 0. 3. 1. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 4. 4. 4. 4. 1. 1. 1. 1. 1. 1. 5. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 4. 4. 4. 4. 1. 1. 1. 5.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 3. 3. 3. 1. 1. 1. 3.]\n",
            "# failures: 0/1000\n",
            "avg. # steps: 12.947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqj6pOIuo8Ol",
        "colab_type": "text"
      },
      "source": [
        "Trung bình thời gian chạy và failures của 2 thuật toán Policy Iteration và Value Iteration ngang nhau\n",
        "\n",
        "Ưu điểm của thuật toán Policy Iteration\n",
        "+ Có thể tìm được node goal\n",
        "+ Đưa ra được hành động tối ưu ở mỗi trạng thái\n",
        "+ Dễ thực hiện\n",
        "\n",
        "Nhược điểm của thuật toán Policy Iteration\n",
        "+ Có thể không tới được node goal\n",
        "+ Độ phức tạp của thuật toán lớn\n",
        "+ Cần một số lần lặp nhất định để đạt trạng thái hội tụ\n",
        "\n",
        "\n"
      ]
    }
  ]
}